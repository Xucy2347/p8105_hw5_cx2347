p8105_hw5_cx2347
================
Chuyuan XU
2025-11-14

``` r
library(tidyverse)
library(stringr)
library(broom)
library(patchwork)
library(rvest)
library(httr)

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = 0.9,
  out.width = "90%"
)
```

### Problem 1

``` r
bd_sim = function(n_obj){
  birthdays = sample(1:365, n_obj, replace = TRUE)
  length(unique(birthdays)) < n_obj
}

sim_results_df = 
  expand_grid(
    sample_size = c(2:50),
    iter = 1:10000
  ) |>
  mutate(
    results = map(sample_size, bd_sim)
  ) |>
  unnest(results) |>
  group_by(sample_size) |>
  summarize(
    n = n(),
    n_true = sum(results == TRUE),
    percent = n_true/n
  )

sim_results_df |>
  ggplot(aes(x = sample_size, y = percent)) + 
  geom_point() +
  geom_line() +
  theme_minimal() +
  scale_y_continuous(
    limits = c(0,1)
  ) +
  labs(
    x = "Sample Size",
    y = "Probability",
    title = "the probability that at least two people in the group will share a birthday"
  )
```

<img src="p8105_hw5_cx2347_files/figure-gfm/Problem 1-1.png" width="90%" />

As the sample size increases, the probability that at least two people
in the group will share a birthday will increase.

### Problem 2

``` r
sim_t.test = function(mu){
  
  sim2_data = tibble(
    x = rnorm(n = 30, mean = mu, sd = 5)
  )
  
  t.test(sim2_data, mu = 0) |>
    broom::tidy() |>
    janitor::clean_names() |>
    select(mu_hat = estimate, p_value)

}

sim_t.test0_results = 
  expand_grid(
    mu = 0,
    iter = 1:5000
  ) |>
  mutate(
    results = map(mu, sim_t.test)
  ) |>
  unnest(results)

sim_t.test_results = 
  expand_grid(
    mu = c(1:6),
    iter = 1:5000
  ) |>
  mutate(
    results = map(mu, sim_t.test)
  ) |>
  unnest(results)
```

``` r
# power vs. true mu
t.test_fig1 = sim_t.test_results |>
  group_by(mu) |>
  summarize(
    power = sum(p_value < 0.05)/5000
  ) |>
  ggplot(aes(x = mu, y = power)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(
    breaks = c(1:6)
  ) + 
  labs(
    x = 'True mu',
    y = 'Power',
    title = 'the power of the test vs. the true value of mu'
  ) + 
  theme_minimal()

t.test_fig1
```

<img src="p8105_hw5_cx2347_files/figure-gfm/Problem 2_plot 1-1.png" width="90%" />

As the True Mu increases, the Power increases.

``` r
# power vs. true mu
t.test_fig2 = sim_t.test_results |>
  group_by(mu) |>
  summarize(
    power = sum(p_value < 0.05)/5000,
    avg_mu_hat = mean(mu_hat), # the average estimate of mu_hat in samples
    avg_mu_hat_rej = mean(mu_hat[p_value < 0.05]) # the average estimate of mu_hat only in samples for which the null was rejected
  ) |>
  pivot_longer(
    names_to = "mu_type",
    values_to = "mu_value",
    cols = c("avg_mu_hat", "avg_mu_hat_rej")
  ) |>
  mutate(
    mu_type = case_match(
      mu_type,
      "avg_mu_hat" ~ "the Mu_hat in Samples",
      "avg_mu_hat_rej" ~ "the Mu_hat in Samples \n(where the null is rejected)"
    )
  ) |>
  ggplot(aes(x = mu, y = mu_value, color = mu_type)) +
  geom_point(color = 'black') +
  geom_line(alpha = 0.8) + 
    scale_x_continuous(
    breaks = c(1:6)
  ) + 
  labs(
    x = 'True Mu',
    y = 'Average Estimated Mean',
    color = "" ,
    title = 'the Average Estimated Mean vs. the True Value of Mean'
  ) +
  theme_minimal()

t.test_fig2
```

<img src="p8105_hw5_cx2347_files/figure-gfm/Problem 2_Plot 2-1.png" width="90%" />

The sample average of mu_hat across tests for which the null is rejected
approximately is equal to the true value of mu after the True Mu reaches
4. When the true mu is greater, the closer two curves are. When the
difference is closer between the true mu and the mu in the null
hypothesis, the effect size and the test power is smaller. Therefore, it
is more unlikely to reject the null hypothesis and only more extreme
means can be used to reject the null hypothesis correctly, which will
lead to a biased observed mean in those groups with smaller true Mu and
a closer value when the true mu is greater.

### Problem 3

``` r
csv = "https://raw.githubusercontent.com/washingtonpost/data-homicides/refs/heads/master/homicide-data.csv"

homicide_raw = read_csv(csv, na=c("NA", "", "."))
```

``` r
# Describe the raw data.
# Create a city_state variable (e.g. “Baltimore, MD”) and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).

homicide_df = homicide_raw |>
  # there is one obs: Tulsa, AL
  # there is no Tulsa, in AL, change it into OK
  mutate(
    state = case_when(
      city == "Tulsa" ~ "OK",
      city == "" ~ "",
      TRUE ~ state
    )
  ) |>
  
  mutate(city_state = str_c(city, ', ', state))

homicide_count1 = homicide_df |>
  group_by(city_state) |>
  summarise(
    n_ttl = n()
  )

homicide_count2 = homicide_df |>
  filter(disposition %in% c("Closed without arrest", "Open/No arrest")) |>
  group_by(city_state) |>
  summarise(
    n_unsolved = n()
  )

homicide_count = left_join(homicide_count1, homicide_count2, by = "city_state") |>
  mutate(city_state = fct_reorder(city_state, desc(n_ttl))) |>
  arrange(city_state)

homicide_count |>
  knitr::kable(
    col.names = c("City, State", "Total Number of homicides", " Total Number of Unsolved Homicides"),
    caption = "The above table presents the total number of homicides and the number of unsolved homicides for 50 cities."
  )
```

| City, State | Total Number of homicides | Total Number of Unsolved Homicides |
|:---|---:|---:|
| Chicago, IL | 5535 | 4073 |
| Philadelphia, PA | 3037 | 1360 |
| Houston, TX | 2942 | 1493 |
| Baltimore, MD | 2827 | 1825 |
| Detroit, MI | 2519 | 1482 |
| Los Angeles, CA | 2257 | 1106 |
| St. Louis, MO | 1677 | 905 |
| Dallas, TX | 1567 | 754 |
| Memphis, TN | 1514 | 483 |
| New Orleans, LA | 1434 | 930 |
| Las Vegas, NV | 1381 | 572 |
| Washington, DC | 1345 | 589 |
| Indianapolis, IN | 1322 | 594 |
| Kansas City, MO | 1190 | 486 |
| Jacksonville, FL | 1168 | 597 |
| Milwaukee, wI | 1115 | 403 |
| Columbus, OH | 1084 | 575 |
| Atlanta, GA | 973 | 373 |
| Oakland, CA | 947 | 508 |
| Phoenix, AZ | 914 | 504 |
| San Antonio, TX | 833 | 357 |
| Birmingham, AL | 800 | 347 |
| Nashville, TN | 767 | 278 |
| Miami, FL | 744 | 450 |
| Cincinnati, OH | 694 | 309 |
| Charlotte, NC | 687 | 206 |
| Oklahoma City, OK | 672 | 326 |
| San Francisco, CA | 663 | 336 |
| Pittsburgh, PA | 631 | 337 |
| New York, NY | 627 | 243 |
| Boston, MA | 614 | 310 |
| Tulsa, OK | 584 | 193 |
| Louisville, KY | 576 | 261 |
| Fort Worth, TX | 549 | 255 |
| Buffalo, NY | 521 | 319 |
| Fresno, CA | 487 | 169 |
| San Diego, CA | 461 | 175 |
| Stockton, CA | 444 | 266 |
| Richmond, VA | 429 | 113 |
| Baton Rouge, LA | 424 | 196 |
| Omaha, NE | 409 | 169 |
| Albuquerque, NM | 378 | 146 |
| Long Beach, CA | 378 | 156 |
| Sacramento, CA | 376 | 139 |
| Minneapolis, MN | 366 | 187 |
| Denver, CO | 312 | 169 |
| Durham, NC | 276 | 101 |
| San Bernardino, CA | 275 | 170 |
| Savannah, GA | 246 | 115 |
| Tampa, FL | 208 | 95 |

The above table presents the total number of homicides and the number of
unsolved homicides for 50 cities.

In the raw dataset `homicide_raw`, there are 52179 observations and 12
variables. The data report the information of the criminal homicides,
whether an arrest was made and, in most cases, basic demographic
information about each victim, in 50 of the largest American cities over
the past decade. Key variables include:  
    `uid`: the identification of each criminal homicide;  
    `reported_date`: the date when the criminal homicide was reported;  
    `victim_last`, `victim_first`, `victim_race`, `victim_age`,
`victim_sex`: basic demographic information about each victim (last
name, first name, race, age, and sex);  
    `city`, `state`, `lat`, `lon`: the geographical information of the
criminal homicide (the city, state, and coordinates where killing
occurred);  
   `disposition`: the status of the case (“Closed without
arrest”/“Closed by arrest”/“Open/No arrest”).  
  
A new variable `city_state` is derived from `city` and `state` through
combination. The `state` of observation for one `city` Tulsa was
recorded as “AL” and was changed back into “OK”.  
  
The above table presents the total number of homicides and the number of
unsolved homicides for each city.

``` r
bal_count = homicide_count |>
  filter(str_detect(city_state, "Baltimore") == TRUE)

# use the prop.test function to estimate the proportion of homicides that are unsolved
# save the output of prop.test as an R object
p.test_bal_raw = prop.test(pull(bal_count, n_unsolved), pull(bal_count, n_ttl)) 

# apply the broom::tidy to this object
# pull the estimated proportion and confidence intervals from the resulting tidy dataframe
p.test_bal = p.test_bal_raw |>
  broom::tidy() |>
  select(estimate, conf.low, conf.high) |>
  mutate(
    city_state = "Baltimore, MD"
  )
```

``` r
city_p.test_results = homicide_count |>
  mutate(
    results = map2(n_unsolved, n_ttl, \(x, y) prop.test(x, y)),
    results = map(results, \(x) broom::tidy(x))
  ) |>
  unnest(results) |>
  select(city_state, estimate, conf.low, conf.high) |>
  mutate(
    city_state = fct_reorder(city_state, estimate)
  ) |>
  arrange(city_state)

# create the plot that shows the estimates and CIs for each city
city_p.test_results |>
  ggplot(aes(x = estimate, y = city_state)) +
  geom_point(
    
  ) +
  geom_errorbar(
    aes(xmin = conf.low, xmax = conf.high),
    width = 0.2,
    alpha = 0.8
  ) +
  labs(
    x = "Estimated Proportion of Unsolved Homicides (with 95% CI)",
    y = "City, State",
    title = "Estimated Proportion of Unsolved Homicides in each City"
  ) + 
  theme_minimal() +
  theme(
    axis.text.y = element_text(size = 8)
  )
```

<img src="p8105_hw5_cx2347_files/figure-gfm/prop.test all cities-1.png" width="90%" />
